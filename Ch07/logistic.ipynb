{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a456ff7-140b-4304-abcd-f448eff4dcb6",
   "metadata": {},
   "outputs": [],
   "source": "# Static variables\nBUCKET='elite-caster-125113'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d224a1-74af-4587-84b1-2bafd96d5519",
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\nspark = SparkSession\\\n  .builder \\\n  .appName(\"Logistic regression w/ Spark ML\") \\\n  .getOrCreate()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f1673-ab02-447e-aca1-27700c82527e",
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.rdd import PipelinedRDD\nimport numpy as np\nfrom collections import namedtuple\nfrom matplotlib import pyplot as plt"
  },
  {
   "cell_type": "markdown",
   "id": "f86ebc78-9a2f-464a-91d1-f4d92395448d",
   "metadata": {},
   "source": "## Creating a Training Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a1404-8e48-4ef1-8efc-aedfd941f710",
   "metadata": {},
   "outputs": [],
   "source": "# CSV to Dataframe\ntraindays: DataFrame = spark.read \\\n  .option(\"header\", \"true\") \\\n  .csv('gs://{}/flights/trainday.csv'.format(BUCKET))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b5748-84bb-4ad1-91e8-283362535167",
   "metadata": {},
   "outputs": [],
   "source": "traindays.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25cbd0-cdc2-4ed6-96da-dbb027ce3fc2",
   "metadata": {},
   "outputs": [],
   "source": "# Register the dataframe as TempView for spark sql\ntraindays.createOrReplaceTempView('traindays')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2605c-160e-4f4b-9afc-51f704d046ec",
   "metadata": {},
   "outputs": [],
   "source": "spark.sql(\"SELECT * FROM traindays LIMIT 5\").show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc91ffe-a4d3-41af-b0d8-8becaedf46b3",
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n\nheader = 'FL_DATE,OP_UNIQUE_CARRIER,OP_CARRIER_AIRLINE_ID,OP_CARRIER,OP_CARRIER_FL_NUM,ORIGIN_AIRPORT_ID,'\nheader += 'ORIGIN_AIRPORT_SEQ_ID,ORIGIN_CITY_MARKET_ID,ORIGIN,DEST_AIRPORT_ID,'\nheader += 'DEST_AIRPORT_SEQ_ID,DEST_CITY_MARKET_ID,DEST,CRS_DEP_TIME,DEP_TIME,'\nheader += 'DEP_DELAY,TAXI_OUT,WHEELS_OFF,WHEELS_ON,TAXI_IN,'\nheader += 'CRS_ARR_TIME,ARR_TIME,ARR_DELAY,CANCELLED,'\nheader += 'CANCELLATION_CODE,DIVERTED,DISTANCE,DEP_AIRPORT_LAT,'\nheader += 'DEP_AIRPORT_LON,DEP_AIRPORT_TZOFFSET,ARR_AIRPORT_LAT,ARR_AIRPORT_LON,'\nheader += 'ARR_AIRPORT_TZOFFSET,EVENT,NOTIFY_TIME'\n\nprint(header)\n\ndef get_structfield(colname: str) -> StructField:\n    if colname in ['ARR_DELAY', 'DEP_DELAY', 'DISTANCE', 'TAXI_OUT']:\n        return StructField(colname, FloatType(), True)\n    else:\n        return StructField(colname, StringType(), True)\n\n\nschema = StructType([get_structfield(colname) for colname in header.split(',')])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b672c23-85ff-4485-b606-dbe17ce93435",
   "metadata": {},
   "outputs": [],
   "source": "inputs = 'gs://{}/flights/tzcorr/flights-00000-*'.format(BUCKET)\n# inputs = 'gs://{}/flights/tzcorr/flights-*'.format(BUCKET)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258e427-df70-46e3-90f7-6518eed1c319",
   "metadata": {},
   "outputs": [],
   "source": "flights: DataFrame = spark.read \\\n  .schema(schema) \\\n  .csv(inputs)\n    \nflights.createOrReplaceTempView('flights')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377a615-40c4-4eab-8399-c97e28e8d907",
   "metadata": {},
   "outputs": [],
   "source": "trainquery: str = \"\"\"\nSELECT\n  f.*\nFROM flights f\nJOIN traindays t\nON f.FL_DATE == t.FL_DATE\nWHERE\n t.is_train_day == 'True'\n\"\"\"\n\ntraindata: DataFrame = spark.sql(trainquery)"
  },
  {
   "cell_type": "markdown",
   "id": "7893db25-1081-457d-87ca-f1b98a2e3684",
   "metadata": {},
   "source": "## Dealing with Corner Cases"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56afa09-f935-4eed-9661-9531c9095921",
   "metadata": {},
   "outputs": [],
   "source": "traindata.head(2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1e140-2632-4d06-94e7-c38a7a23a8af",
   "metadata": {},
   "outputs": [],
   "source": "traindata[[\"DEP_DELAY\", \"TAXI_OUT\", \"ARR_DELAY\", \"DISTANCE\"]].describe().show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2c281-99f8-490e-80ac-5ed03220ec48",
   "metadata": {},
   "outputs": [],
   "source": "# Revise query by putting NULL fields into account\n# Flights that were scheduled but \n#   never left the gate (DEP_DELAY is null)\n#   never take off (TAXI_OUT is null) \n# Flights took off but diverted and do not have an ARR_DELAY (This includes TAXI_OUT)\ntrainquery_revised: str = \"\"\"\nSELECT\n  DEP_DELAY, TAXI_OUT, ARR_DELAY, DISTANCE\nFROM flights f\nJOIN traindays t\nON f.FL_DATE == t.FL_DATE\nWHERE\n t.is_train_day == 'True' AND\n f.DEP_DELAY IS NOT NULL AND\n f.ARR_DELAY IS NOT NULL\n\"\"\"\n    \ntraindata: DataFrame = spark.sql(trainquery_revised)\ntraindata.describe().show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f828d83-ded6-440e-8095-e551a1246315",
   "metadata": {},
   "outputs": [],
   "source": "# I want to fix the root cause instead of fixing the symptom\n# See if there are really no NULLs\ntrainquery_revised_test: str = \"\"\"\nSELECT\n  DEP_DELAY, TAXI_OUT, ARR_DELAY, DISTANCE\nFROM flights f\nJOIN traindays t\nON f.FL_DATE == t.FL_DATE\nWHERE\n t.is_train_day == 'True' AND\n f.CANCELLED == '0.00' AND\n f.DIVERTED == '0.00' AND\n (f.DEP_DELAY IS NULL) OR\n (f.ARR_DELAY IS NULL)\n\"\"\"\n    \ntraindata: DataFrame = spark.sql(trainquery_revised_test)\ntraindata.head(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb5d2d-1e40-4f82-b1d9-7573b0f0518f",
   "metadata": {},
   "outputs": [],
   "source": "# Lookds like there still NULLs although we have excluded CACELLED and DIVERTED flights.\n# Note: In the book it says that counts will be the same but in my caee it was not so I still needed to exclude the NULLs.\ntrainquery_revised_final: str = \"\"\"\nSELECT\n  DEP_DELAY, TAXI_OUT, ARR_DELAY, DISTANCE\nFROM flights f\nJOIN traindays t\nON f.FL_DATE == t.FL_DATE\nWHERE\n t.is_train_day == 'True' AND\n f.CANCELLED == '0.00' AND\n f.DIVERTED == '0.00' AND\n f.DEP_DELAY IS NOT NULL AND\n f.ARR_DELAY IS NOT NULL\n\"\"\"\n    \ntraindata: DataFrame = spark.sql(trainquery_revised_final)\ntraindata.describe().show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "ace23e14-dc20-442e-a3be-50407985c559",
   "metadata": {},
   "source": "## Creating Training Examples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c776609-8efe-48d1-ba2f-a14e73ac35ff",
   "metadata": {},
   "outputs": [],
   "source": "# To use Logistic Regression (https://bit.ly/3HGBYpw)\n# I first need labled training sets for binary outcomes.\n# In this case , positive lable (1) and negative label(0)\n# Note: https://spark.apache.org/docs/3.1.1/mllib-linear-methods.html#loss-functions\n# Note that, in the mathematical formulation above, a binary label y is denoted as either +1 (positive) or −1 (negative), \n# which is convenient for the formulation. \n# However, the negative label is represented by 0 in spark.mllib instead of −1, to be consistent with multiclass labeling.\ndef to_example(raw_data_point: DataFrame) -> LabeledPoint:\n    return LabeledPoint(\\\n            float(raw_data_point['ARR_DELAY'] < 15),  # on-time? \\\n            [ \\\n                raw_data_point['DEP_DELAY'], \\\n                raw_data_point['TAXI_OUT'], \\\n                raw_data_point['DISTANCE'], \\\n            ])\n\nexamples: DataFrame = traindata.rdd.map(to_example)"
  },
  {
   "cell_type": "markdown",
   "id": "67f7247c-925e-4df9-913c-4c8dad658266",
   "metadata": {},
   "source": "## Training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e87f9-2c6b-4d9e-bc1c-a15aa388b527",
   "metadata": {},
   "outputs": [],
   "source": "# Creating a model means finding out the weights\n# w0*x0 + w1*x1 + w2*x2 + b\nlrmodel: LogisticRegressionModel = LogisticRegressionWithLBFGS.train(examples, intercept=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f935dd3-9aca-464a-a069-0fd7e8b79a4c",
   "metadata": {},
   "outputs": [],
   "source": "print(lrmodel.weights,lrmodel.intercept)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a63293-4bb3-4da7-9358-68b019c585ac",
   "metadata": {},
   "outputs": [],
   "source": "# DEP_DELAY, TAXI_OUT, DISTANCE\nlrmodel.predict([6.0, 12.0, 594.0])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017b7a3-9321-4edc-80fa-bfca5bcf6ade",
   "metadata": {},
   "outputs": [],
   "source": "lrmodel.predict([36.0, 12.0, 594.0])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f20c78-ab4b-4a29-80fe-1f753854f088",
   "metadata": {},
   "outputs": [],
   "source": "lrmodel.clearThreshold()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be74bd-f147-40ec-bcef-452a2704fedf",
   "metadata": {},
   "outputs": [],
   "source": "# Predict probability with fixed dep delay and taxi-out\ndist: np.ndarray = np.arange(10, 2000, 10)\nprob: list = [lrmodel.predict([20,10,d]) for d in dist]\nplt.plot(dist, prob)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae81099-65cc-478e-bd8e-db81f94913fe",
   "metadata": {},
   "outputs": [],
   "source": "# Predict probability with fixed taxi-out  and distance\ndelay: np.ndarray = np.arange(-20, 60, 1)\nprob= list = [lrmodel.predict([d, 10, 500]) for d in delay]\nax = plt.plot(delay, prob)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e586eac-213c-453b-818d-b71cb902c94e",
   "metadata": {},
   "outputs": [],
   "source": "lrmodel.setThreshold(0.7)"
  },
  {
   "cell_type": "markdown",
   "id": "a3d11261-ba01-427d-adc4-0b4ed35d9b51",
   "metadata": {},
   "source": "# Predicting by Using a Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8857866-8f3f-4540-a471-de3e62c9950f",
   "metadata": {},
   "outputs": [],
   "source": "# Save model to cloud stroage for future use\nMODEL_FILE: str = f\"gs://{BUCKET}/flights/sparkmloutput/model\"\nlrmodel.save(sc, MODEL_FILE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6353a-4e8c-4a22-85ac-3fe15eec243e",
   "metadata": {},
   "outputs": [],
   "source": "# Predict from saved model in google storage\nfrom pyspark.mllib.classification import LogisticRegressionModel\nlrmodel: LogisticRegressionModel = LogisticRegressionModel.load(sc, MODEL_FILE)\nlrmodel.setThreshold(0.7)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710a2c3",
   "metadata": {},
   "outputs": [],
   "source": "print(lrmodel.predict([36.0, 12.0, 594.0]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e1fa4",
   "metadata": {},
   "outputs": [],
   "source": "# Use non test days for evaluation\ntestquery: str = trainquery_revised_final.replace(\"t.is_train_day == 'True'\", \"t.is_train_day == 'False'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea4522",
   "metadata": {},
   "outputs": [],
   "source": "print(testquery)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbcc73",
   "metadata": {},
   "outputs": [],
   "source": "# Label the dataset\ntestdata: DataFrame = spark.sql(testquery)\nexamples: DataFrame = testdata.rdd.map(to_example)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b003447",
   "metadata": {},
   "outputs": [],
   "source": "# Predict\nlabelPred: namedtuple = namedtuple('labelPred', [\"label\", \"pred\"])\nlabelpred: PipelinedRDD = examples.map(lambda p: labelPred(p.label, lrmodel.predict(p.features)))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008cfb5",
   "metadata": {},
   "outputs": [],
   "source": "# Layout statistics for evaluation\ndef eval(labelpred: namedtuple):\n    cancel: PipelinedRDD = labelpred.filter(lambda x: x.pred== 1)\n    nocancel: PipelinedRDD = labelpred.filter(lambda x: x.pred == 0)\n\n    corr_cancel = cancel.filter(lambda x: x.label == x.pred).count()\n    corr_nocancel = nocancel.filter(lambda x: x.label == x.pred).count()\n    \n    \n    print(f\"Predicted as canceled: {cancel.count()}\")\n    print(f\"Predicted as noncancel: {nocancel.count()}\")\n    print(f\"Correctly labeled canceled: {corr_cancel}\")\n    print(f\"Correctly labled not canceled: {corr_nocancel}\")\n    \n    return {\n        'total_cancel': cancel.count(),\n        'correct_cancel': float(corr_cancel)/cancel.count(),\n        'total_nocancel': nocancel.count(),\n        'correct_nocancel': float(corr_nocancel)/nocancel.count()\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdf27e",
   "metadata": {},
   "outputs": [],
   "source": "print(eval(labelpred))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd039fb",
   "metadata": {},
   "outputs": [],
   "source": "# Revised evaluation. Based no porbability instead of categorical prediction.\ndef eval_revised(labelpred: namedtuple):\n    cancel: PipelinedRDD = labelpred.filter(lambda x: x.pred < 0.7)\n    nocancel: PipelinedRDD = labelpred.filter(lambda x: x.pred >= 0.7)\n\n    corr_cancel = cancel.filter(lambda x: x.label == int(x.pred < 0.7)).count()\n    corr_nocancel = nocancel.filter(lambda x: x.label == int(x.pred >= 0.7)).count()\n    \n    \n    print(f\"Predicted as canceled: {cancel.count()}\")\n    print(f\"Predicted as noncancel: {nocancel.count()}\")\n    print(f\"Correctly labeled canceled: {corr_cancel}\")\n    print(f\"Correctly labled not canceled: {corr_nocancel}\")\n    \n    return {\n        'total_cancel': cancel.count(),\n        'correct_cancel': float(corr_cancel)/cancel.count(),\n        'total_nocancel': nocancel.count(),\n        'correct_nocancel': float(corr_nocancel)/nocancel.count()\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380843c9",
   "metadata": {},
   "outputs": [],
   "source": "lrmodel.clearThreshold() # so it returns probabilities\n\n# Predict\nlabelPred: namedtuple = namedtuple('labelPred', [\"label\", \"pred\"])\nlabelpred: PipelinedRDD = examples.map(lambda p: labelPred(p.label, lrmodel.predict(p.features)))\n\nprint(eval_revised(labelpred))\n\n# keep only those examples near the decision threshold\nlabelpred: PipelinedRDD = labelpred.filter(lambda p: p.pred > 0.65 and p.pred < 0.75)\nprint(eval_revised(labelpred))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}